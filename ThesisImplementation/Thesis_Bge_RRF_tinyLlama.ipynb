{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d46650",
   "metadata": {},
   "source": [
    "### Initial Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c9ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Environment installation complete. running on CPU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*RobertaModel.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pooler.dense.*\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    " \n",
    "device='cpu'\n",
    "print(\"Environment installation complete. running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49502d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d2242df940408a9961d82171c1e263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c01225036a4f7b84df36f7b1518115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 questions and answers\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#Load the dataset\n",
    "dataset = load_dataset(\"trivia_qa\", \"rc\", split='train[:10000]')\n",
    "\n",
    "#Extract questions and Answers\n",
    "questions = [item['question'] for item in dataset]\n",
    "answers = [item['answer'] for item in dataset]\n",
    "\n",
    "print(f'Loaded {len(questions)} questions and answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d95ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"Total answers: {len(answers)}\")\\nprint(f\"Sample of first 5 answers:\")\\nfor i, answer in enumerate(answers[:5]):\\n    print(f\"  Answer {i}: Type={type(answer)}, Value={answer}\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(f\"Total answers: {len(answers)}\")\n",
    "print(f\"Sample of first 5 answers:\")\n",
    "for i, answer in enumerate(answers[:5]):\n",
    "    print(f\"  Answer {i}: Type={type(answer)}, Value={answer}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b82067c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"Total Questions: {len(questions)}\")\\nprint(f\"Sample of first 5 questions:\")\\nfor i, question in enumerate(questions[150:300]):\\n    print(f\"  Question {i}: Type={type(question)}, Value={question}\")\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(f\"Total Questions: {len(questions)}\")\n",
    "print(f\"Sample of first 5 questions:\")\n",
    "for i, question in enumerate(questions[150:300]):\n",
    "    print(f\"  Question {i}: Type={type(question)}, Value={question}\")\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f14ab",
   "metadata": {},
   "source": [
    "### Set up Database and Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cabca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from evaluate import load\n",
    "import torch\n",
    "#Set pytorch seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#initialize chromadb client\n",
    "chroma_client = chromadb.PersistentClient('./chroma_db_optimized')\n",
    "\n",
    "#embedding function\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "             model_name=\"BAAI/bge-small-en-v1.5\",device='cpu',\n",
    "             trust_remote_code=True    \n",
    "             )\n",
    "\n",
    "#create collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name = 'trivia-qa',\n",
    "    embedding_function= embedding_function,\n",
    "    metadata={\"hnsw:space\":\"cosine\",\n",
    "             \"hnsw:construction_ef\":100,\n",
    "             \"hnsw:search_ef\":50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0da27049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Language Model....\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "print(\"Loading Language Model....\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",trust_remote_code=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",trust_remote_code=True)\n",
    "#set seed for transformers if available\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "    set_seed(SEED)\n",
    "except ImportError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df998d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121b77b",
   "metadata": {},
   "source": [
    "### Generate Embeddings and populate the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0cfce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate embeddings and populate vector database\n",
      "Processed 0 items\n",
      "Processed 1000 items\n",
      "Processed 2000 items\n",
      "Processed 3000 items\n",
      "Processed 4000 items\n",
      "Processed 5000 items\n",
      "Processed 6000 items\n",
      "Processed 7000 items\n",
      "Processed 8000 items\n",
      "Processed 9000 items\n",
      "Database population complete.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_answer_value(answer_dict):\n",
    "    \"\"\"Extract the primary answer value\"\"\"\n",
    "    if isinstance(answer_dict,dict):\n",
    "        if 'normalized_value' in answer_dict and answer_dict['normalized_value']:\n",
    "            return answer_dict['normalized_value']\n",
    "        if 'value' in answer_dict and answer_dict['value']:\n",
    "            return answer_dict['value']\n",
    "        if 'aliases' in answer_dict and answer_dict['aliases']:\n",
    "            return answer_dict['aliases'][0]\n",
    "    return str(answer_dict)\n",
    "\n",
    "print(\"Generate embeddings and populate vector database\")\n",
    "batch_size = 500 #process in batches to manage memory\n",
    "\n",
    "for i in range(0,len(questions), batch_size):\n",
    "    batch_questions = questions[i:i+batch_size]\n",
    "    batch_answers = answers[i:i+batch_size]\n",
    "    \n",
    "    #convert answers to strings\n",
    "    batch_answer_strings = [extract_answer_value(answer) for answer in batch_answers]\n",
    "    \n",
    "    #generate embeddings for this batch of questions\n",
    "    embeddings = embedding_function(batch_questions)\n",
    "    \n",
    "    #Create IDs for each document\n",
    "    ids = [f\"id_{j}\" for j in range(i,min(i+batch_size,len(questions)))]\n",
    "    \n",
    "    #Add to collection\n",
    "    collection.add(\n",
    "    embeddings= embeddings,\n",
    "    documents= batch_answer_strings,\n",
    "    metadatas= [{\"questions\" : q} for q in batch_questions],\n",
    "    ids=ids\n",
    "    )\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f'Processed {i} items')\n",
    "        \n",
    "print(\"Database population complete.\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df8fc5",
   "metadata": {},
   "source": [
    "### RAG PIPELINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe7bd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Disable gradients for inference\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "# Using a lightweight model for CPU environments\n",
    "embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "class HybridReasoningRetriever:\n",
    "    def __init__(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid retriever with both sparse and dense components.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents to retrieve from\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Initialize sparse retriever (BM25)\n",
    "        self.tokenize = lambda text: re.findall(r'\\w+', text.lower())\n",
    "        tokenized_corpus = [self.tokenize(doc) for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        \n",
    "        # Precompute document embeddings for dense retrieval\n",
    "        self.doc_embeddings = embedding_model.encode(documents, show_progress_bar=False)\n",
    "        \n",
    "        # Initialize query expansion model (would use a lightweight LLM in practice)\n",
    "        self.expansion_model = None  # Placeholder for a small LLM\n",
    "    \n",
    "    def sparse_retrieve(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents using BM25 (sparse retrieval).\n",
    "        \"\"\"\n",
    "        tokenized_query = self.tokenize(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        return [(i, scores[i]) for i in top_indices]\n",
    "    \n",
    "    def dense_retrieve(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents using dense embeddings.\n",
    "        \"\"\"\n",
    "        query_embedding = embedding_model.encode([query])\n",
    "        similarities = np.dot(self.doc_embeddings, query_embedding.T).flatten()\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        return [(i, similarities[i]) for i in top_indices]\n",
    "    \n",
    "\n",
    "    \n",
    "    def hybrid_retrieve(self, query: str, top_k: int = 5, \n",
    "                       alpha: float = 0.7) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Hybrid retrieval combining sparse and dense methods.\n",
    "        \n",
    "        Args:\n",
    "            query: The query to retrieve documents for\n",
    "            top_k: Number of documents to retrieve\n",
    "            alpha: Weight for dense retrieval (1-alpha for sparse)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        retrieval_query = query\n",
    "\n",
    "        \n",
    "        # Get sparse results\n",
    "        sparse_results = self.sparse_retrieve(retrieval_query, top_k * 2)\n",
    "        sparse_scores = np.zeros(len(self.documents))\n",
    "        for idx, score in sparse_results:\n",
    "            sparse_scores[idx] = score\n",
    "        \n",
    "        # Get dense results\n",
    "        dense_results = self.dense_retrieve(retrieval_query, top_k * 2)\n",
    "        dense_scores = np.zeros(len(self.documents))\n",
    "        for idx, score in dense_results:\n",
    "            dense_scores[idx] = score\n",
    "        \n",
    "        # Normalize scores\n",
    "        if np.max(sparse_scores) > 0:\n",
    "            sparse_scores = sparse_scores / np.max(sparse_scores)\n",
    "        if np.max(dense_scores) > 0:\n",
    "            dense_scores = dense_scores / np.max(dense_scores)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = alpha * dense_scores + (1 - alpha) * sparse_scores\n",
    "        top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "        \n",
    "        return [(i, combined_scores[i]) for i in top_indices]\n",
    "\n",
    "def retriever(query: str, k: int = 3, score_threshold: float = 0.4,\n",
    "             max_candidates: int = 30) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Enhanced hybrid retriever with reasoning capabilities.\n",
    "    \n",
    "    Args:\n",
    "        query: Trivia question to search for\n",
    "        k: Number of relevant answers to retrieve\n",
    "        score_threshold: Minimum similarity score to include a result\n",
    "        max_candidates: Maximum number of candidates to consider\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing document content, metadata, and similarity score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        global hybrid_retriever\n",
    "        \n",
    "        # Preprocess query\n",
    "        processed_query = _preprocess_trivia_query(query)\n",
    "        \n",
    "        # Use hybrid retrieval with reasoning capabilities\n",
    "        results = hybrid_retriever.hybrid_retrieve(\n",
    "            processed_query, \n",
    "            top_k=min(k * 5, max_candidates),\n",
    "        )\n",
    "        \n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        # Convert to scored results format\n",
    "        scored_results = []\n",
    "        for idx, score in results:\n",
    "            scored_results.append({\n",
    "                'content': hybrid_retriever.documents[idx],\n",
    "                'score': float(score)  # Convert numpy float to Python float\n",
    "            })\n",
    "        \n",
    "        # Filter by score threshold\n",
    "        filtered_results = [r for r in scored_results if r['score'] >= score_threshold]\n",
    "        \n",
    "        # Filter low-quality context\n",
    "        filtered_results = _filter_low_quality_context(filtered_results)\n",
    "        \n",
    "        # If no results meet threshold, return top results\n",
    "        if not filtered_results and scored_results:\n",
    "            filtered_results = scored_results[:min(3, len(scored_results))]\n",
    "        \n",
    "        # Ensure scores meet minimum threshold\n",
    "        for result in filtered_results:\n",
    "            result['score'] = max(score_threshold - 0.1, result['score'])\n",
    "        \n",
    "        # Sort by score\n",
    "        filtered_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Deduplicate\n",
    "        unique_results = _deduplicate(filtered_results, k)\n",
    "        \n",
    "        return unique_results[:k]\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "# Keep existing helper functions\n",
    "def _preprocess_trivia_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess the query to focus on key entities and facts\n",
    "    \"\"\"\n",
    "    words = query.lower().split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in ['who', 'when', 'where', 'which', 'what']:\n",
    "            filtered_words.append(word)\n",
    "        elif word not in ['is', 'are', 'did', 'do', 'does', 'the', 'a', 'an']:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    return \" \".join(filtered_words) if filtered_words else query\n",
    "\n",
    "def _deduplicate(results: List[Dict], k: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Memory-efficient deduplication for resource-constrained environments\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "    \n",
    "    unique_results = []\n",
    "    seen_content_hashes = set()\n",
    "    \n",
    "    for result in results:\n",
    "        if len(unique_results) >= k * 2:\n",
    "            break\n",
    "        \n",
    "        content = result['content']\n",
    "        content_hash = _simple_content_hash(content)\n",
    "        \n",
    "        if content_hash not in seen_content_hashes:\n",
    "            seen_content_hashes.add(content_hash)\n",
    "            unique_results.append(result)\n",
    "    \n",
    "    return unique_results\n",
    "\n",
    "def _simple_content_hash(content: str, max_length: int = 200) -> int:\n",
    "    \"\"\"\n",
    "    Create a simple hash for content deduplication without heavy processing\n",
    "    \"\"\"\n",
    "    short_content = content[:max_length] if len(content) > max_length else content\n",
    "    return hash(short_content)\n",
    "\n",
    "def _filter_low_quality_context(contexts: List[Dict], min_length: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filter out low-quality context documents\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for ctx in contexts:\n",
    "        content = ctx['content']\n",
    "        \n",
    "        if len(content.strip()) < min_length:\n",
    "            continue\n",
    "            \n",
    "        # Skip JSON-like content\n",
    "        if content.strip().startswith('{') and '}' in content:\n",
    "            continue\n",
    "            \n",
    "        # Skip content with too many special characters\n",
    "        if sum(1 for c in content if not c.isalnum() and not c.isspace()) / len(content) > 0.5:\n",
    "            continue\n",
    "            \n",
    "        filtered.append(ctx)\n",
    "        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a20ef588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing hybrid retriever with documents...\n",
      "Loaded 10000 documents for hybrid retriever\n",
      "Hybrid retriever initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the hybrid retriever with your actual documents\n",
    "print(\"Initializing hybrid retriever with documents...\")\n",
    "\n",
    "# Extract the actual documents from your collection\n",
    "document_contents = []\n",
    "batch_size = 500\n",
    "\n",
    "for i in range(0, len(questions), batch_size):\n",
    "    batch_end = min(i + batch_size, len(questions))\n",
    "    ids = [f\"id_{j}\" for j in range(i, batch_end)]\n",
    "    \n",
    "    try:\n",
    "        results = collection.get(ids=ids)\n",
    "        document_contents.extend(results['documents'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving batch {i}-{batch_end}: {e}\")\n",
    "        # Fallback: use the original answer strings\n",
    "        document_contents.extend([json.dumps(answer) for answer in answers[i:batch_end]])\n",
    "\n",
    "print(f\"Loaded {len(document_contents)} documents for hybrid retriever\")\n",
    "\n",
    "# Initialize the hybrid retriever (GLOBAL)\n",
    "hybrid_retriever = HybridReasoningRetriever(document_contents)\n",
    "print(\"Hybrid retriever initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06c8135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "def create_rag_prompt(question, contexts, system_msg=\"You are a factual question-answering assistant.\"):\n",
    "    \"\"\"Improved prompt construction for trivia QA\"\"\"\n",
    "    \n",
    "    # Format contexts properly\n",
    "    context_text = \"\"\n",
    "    for i, ctx in enumerate(contexts[:3]):  # Use top 3 contexts max\n",
    "        if isinstance(ctx, dict):\n",
    "            content = ctx.get('content', '')\n",
    "        else:\n",
    "            content = str(ctx)\n",
    "        \n",
    "        # Clean and truncate context\n",
    "        clean_ctx = content.strip()\n",
    "        if len(clean_ctx) > 300:  # Truncate very long contexts\n",
    "            clean_ctx = clean_ctx[:300] + \"...\"\n",
    "        \n",
    "        context_text += f\"[Context {i+1}]: {clean_ctx}\\n\\n\"\n",
    "    \n",
    "    return f\"\"\"{system_msg}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "RELEVANT CONTEXTS:\n",
    "{context_text}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Answer based ONLY on the provided contexts\n",
    "- Be concise and factual\n",
    "- If the answer is not in the contexts, say \"I cannot find the answer in the provided information\"\n",
    "- Do not make up information\n",
    "- Keep your answer short (1-3 words when possible)\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "def generator(question, contexts, llm_model, tokenizer):\n",
    "    \"\"\"\n",
    "    Improved generator function with better context selection and processing\n",
    "    \"\"\"\n",
    "    \n",
    "    if not contexts:\n",
    "        return \"I cannot find the answer in the provided information.\"\n",
    "    \n",
    "    # Debug: Check what contexts look like\n",
    "    print(f\"Number of contexts received: {len(contexts)}\")\n",
    "    if contexts:\n",
    "        print(f\"First context type: {type(contexts[0])}\")\n",
    "        print(f\"First context keys (if dict): {contexts[0].keys() if isinstance(contexts[0], dict) else 'Not a dict'}\")\n",
    "    \n",
    "    # Extract content from contexts if they are dictionaries\n",
    "    processed_contexts = []\n",
    "    for ctx in contexts:\n",
    "        if isinstance(ctx, dict):\n",
    "            # Handle different possible content structures\n",
    "            if 'content' in ctx:\n",
    "                content = ctx['content']\n",
    "            elif 'text' in ctx:\n",
    "                content = ctx['text']\n",
    "            elif 'value' in ctx:\n",
    "                content = ctx['value']\n",
    "            else:\n",
    "                # If it's a string-like dict, use the string representation\n",
    "                content = str(ctx)\n",
    "        else:\n",
    "            content = str(ctx)\n",
    "        \n",
    "        processed_contexts.append(content)\n",
    "    \n",
    "    # Improved context selection with simple ranking\n",
    "    if len(processed_contexts) > 1:\n",
    "        ranked_contexts = rank_contexts(question, processed_contexts)\n",
    "    else:\n",
    "        ranked_contexts = processed_contexts\n",
    "    \n",
    "    # Construct prompt with top contexts\n",
    "    final_prompt = create_rag_prompt(question, ranked_contexts[:3])  # Use top 3 contexts\n",
    "    \n",
    "    # Tokenization with better parameters\n",
    "    inputs = tokenizer(\n",
    "        final_prompt,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=512,  # Reduced for better performance\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Improved generation config for factual answers\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=50,  # Increased for complete answers\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.1,  # Lower temperature for more factual responses\n",
    "        do_sample=False,  # Use greedy sampling for factual answers\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    llm_model.eval()\n",
    "    \n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = llm_model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                generation_config=generation_config,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            # Extract only the generated part\n",
    "            generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "            answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Clean up the answer\n",
    "            answer = clean_answer(answer)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            answer = \"I cannot provide an answer at the moment.\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def rank_contexts(question, contexts):\n",
    "    \"\"\"\n",
    "    Simple but effective context ranking for trivia QA\n",
    "    \"\"\"\n",
    "    if not contexts:\n",
    "        return []\n",
    "    \n",
    "    question_lower = question.lower()\n",
    "    question_words = set(re.findall(r'\\w+', question_lower))\n",
    "    \n",
    "    scored_contexts = []\n",
    "    \n",
    "    for i, context in enumerate(contexts):\n",
    "        score = 0.0\n",
    "        \n",
    "        # Basic length score (prefer medium-length contexts)\n",
    "        context_length = len(context)\n",
    "        if 50 <= context_length <= 500:  # Ideal length range\n",
    "            score += 1.0\n",
    "        elif context_length < 20:  # Too short\n",
    "            score -= 2.0\n",
    "        \n",
    "        # Keyword overlap score\n",
    "        context_lower = context.lower()\n",
    "        context_words = set(re.findall(r'\\w+', context_lower))\n",
    "        overlap = len(question_words.intersection(context_words))\n",
    "        keyword_score = overlap / max(len(question_words), 1)\n",
    "        score += keyword_score * 2.0  # Weight keyword matches heavily\n",
    "        \n",
    "        # Exact phrase matching (important for trivia)\n",
    "        if any(word in context_lower for word in question_lower.split()):\n",
    "            score += 1.5\n",
    "        \n",
    "        # Position bonus (often first contexts are better)\n",
    "        score += (1.0 / (i + 1)) * 0.5\n",
    "        \n",
    "        scored_contexts.append((context, score))\n",
    "    \n",
    "    # Sort by score descending\n",
    "    scored_contexts.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [ctx for ctx, score in scored_contexts]\n",
    "\n",
    "def clean_answer(answer):\n",
    "    \"\"\"\n",
    "    Clean and format the answer for trivia questions\n",
    "    \"\"\"\n",
    "    if not answer:\n",
    "        return \"I cannot find the answer in the provided information.\"\n",
    "    \n",
    "    # Remove any prompt fragments that might have been generated\n",
    "    unwanted_phrases = [\n",
    "        \"ANSWER:\", \"Answer:\", \"based on the context\", \"according to the text\",\n",
    "        \"the context says\", \"the information states\"\n",
    "    ]\n",
    "    \n",
    "    for phrase in unwanted_phrases:\n",
    "        answer = answer.replace(phrase, \"\")\n",
    "    \n",
    "    # Clean up punctuation and whitespace\n",
    "    answer = re.sub(r'[^\\w\\s\\.\\,\\-\\']', '', answer)  # Keep basic punctuation\n",
    "    answer = answer.strip()\n",
    "    \n",
    "    # Capitalize first letter\n",
    "    if answer and answer[0].islower():\n",
    "        answer = answer[0].upper() + answer[1:]\n",
    "    \n",
    "    # Handle empty answers\n",
    "    if not answer or len(answer) < 2:\n",
    "        return \"I cannot find the answer in the provided information.\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Optional: Add a simple evaluation helper\n",
    "def evaluate_answer(true_answer, generated_answer):\n",
    "    \"\"\"\n",
    "    Simple evaluation for debugging\n",
    "    \"\"\"\n",
    "    true_lower = true_answer.lower()\n",
    "    generated_lower = generated_answer.lower()\n",
    "    \n",
    "    # Exact match\n",
    "    if true_lower == generated_lower:\n",
    "        return 1.0\n",
    "    \n",
    "    # Substring match\n",
    "    if true_lower in generated_lower or generated_lower in true_lower:\n",
    "        return 0.8\n",
    "    \n",
    "    # Keyword overlap\n",
    "    true_words = set(re.findall(r'\\w+', true_lower))\n",
    "    gen_words = set(re.findall(r'\\w+', generated_lower))\n",
    "    overlap = len(true_words.intersection(gen_words))\n",
    "    \n",
    "    if overlap >= len(true_words) * 0.5:  # 50% keyword overlap\n",
    "        return 0.6\n",
    "    \n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0ba7a4",
   "metadata": {},
   "source": [
    "## Functions to Calculate Answer Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7324a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "def calculate_semantic_similarity(text1:str, text2:str)-> float:\n",
    "    if not text1 or not text2:\n",
    "        return 0.0\n",
    "    \n",
    "    if model is not None:\n",
    "        try:\n",
    "            embeddings1 = model.encode(text1,convert_to_tensor=True)\n",
    "            embeddings2 = model.encode(text2,convert_to_tensor=True)\n",
    "            cosine_scores = util.pytorch_cos_sim(embeddings1,embeddings2)\n",
    "            return float(cosine_scores[0][0])\n",
    "        except Exception as e:\n",
    "            print(f\"Semantic similarity calculation failed : {e}\")\n",
    "\n",
    "def calculate_simple_similarity(text1:str, text2:str)-> float:\n",
    "    words1 = set(text1.split())\n",
    "    words2 = set(text2.split())\n",
    "    \n",
    "    if not words1 or not words2:\n",
    "        return 0.0\n",
    "    return len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "def calculate_correctness(substring_match,semantic_similarity,bert_f1_score):\n",
    "    substring_match_bool = bool(substring_match)\n",
    "    semantic_sim_norm = max(0.0,min(1.0,semantic_similarity))\n",
    "    bert_f1_norm = max(0.0,min(1.0,bert_f1_score))\n",
    "    \n",
    "    if substring_match_bool:\n",
    "        base_score = 0.80\n",
    "        bonus = 0.20 * (0.5 * semantic_sim_norm + 0.5 * bert_f1_norm)\n",
    "        return min(1.0,base_score + bonus)\n",
    "    else:\n",
    "        semantic_weight = 0.6\n",
    "        bert_f1_weight = 0.4\n",
    "        composite_score = (semantic_weight * semantic_sim_norm + bert_f1_weight * bert_f1_norm)\n",
    "        final_score = composite_score ** 0.8\n",
    "        return final_score\n",
    "    \n",
    "def normalize_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_answer_relevance(question: str, generated_answer: str, \n",
    "                             ground_truth_answer: str = None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate how relevant the answer is to the question\n",
    "    \"\"\"\n",
    "    if not question or not generated_answer:\n",
    "        return {\"relevance_score\": 0.0, \"question_similarity\": 0.0}\n",
    "    \n",
    "    # Method 1: Direct similarity between question and answer\n",
    "    question_answer_similarity = calculate_semantic_similarity(question, generated_answer)\n",
    "    \n",
    "    # Method 2: If ground truth is available, compare answer similarity patterns\n",
    "    if ground_truth_answer:\n",
    "        gt_question_similarity = calculate_semantic_similarity(question, ground_truth_answer)\n",
    "        answer_gt_similarity = calculate_semantic_similarity(generated_answer, ground_truth_answer)\n",
    "        \n",
    "        # Relevance score combines direct similarity and alignment with ground truth pattern\n",
    "        relevance_score = 0.7 * question_answer_similarity + 0.3 * answer_gt_similarity\n",
    "    else:\n",
    "        relevance_score = question_answer_similarity\n",
    "    \n",
    "    return {\n",
    "        \"relevance_score\": min(1.0, relevance_score),\n",
    "        \"question_similarity\": question_answer_similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "884d4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from bert_score import score as bert_score\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import torch\n",
    "\n",
    "# Suppress all transformers warnings\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# More comprehensive warning suppression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"bert_score\").setLevel(logging.ERROR)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Force CPU usage\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def evaluate_rag_system(test_questions, test_answers, llm_model, tokenizer, k=3):\n",
    "    \"\"\"\n",
    "    test_questions (list[str]): Input questions\n",
    "    test_answers (list[str]): Ground truth answers\n",
    "    llm_model: Preloaded language model (CPU)\n",
    "    tokenizer: Preloaded tokenizer\n",
    "    k (int): Top-k retrieved documents\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    latencies = []\n",
    "    cpu_times = []\n",
    "    memory_usages = []\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    for i, (question, true_answer) in enumerate(zip(test_questions, test_answers)):\n",
    "        if i % 10 == 0:\n",
    "            print(f'Processing question {i}/{len(test_questions)}')\n",
    "            \n",
    "        # Start time\n",
    "        start_time = time.time()\n",
    "        cpu_start = process.cpu_times()\n",
    "        mem_start = process.memory_info().rss\n",
    "        \n",
    "        # Retrieve Context and generate answer\n",
    "        contexts_dicts = retriever(question, k=k)\n",
    "        \n",
    "        # Generate Answers\n",
    "        answer = generator(question, contexts_dicts,llm_model,tokenizer)\n",
    "        \n",
    "        # End timing and calculate metrics\n",
    "        end_time = time.time()\n",
    "        cpu_end = process.cpu_times()\n",
    "        mem_end = process.memory_info().rss\n",
    "        \n",
    "        latency = end_time - start_time\n",
    "        cpu_time = (cpu_end.user - cpu_start.user) + (cpu_end.system - cpu_start.system)\n",
    "        memory_usage = (mem_end - mem_start) / 1024 / 1024  # Convert to MB\n",
    "        \n",
    "        predictions.append(answer)\n",
    "        latencies.append(latency)\n",
    "        cpu_times.append(cpu_time)\n",
    "        memory_usages.append(max(0, memory_usage))\n",
    "\n",
    "    all_results = []\n",
    "    relevance_metrics=[]\n",
    "        \n",
    "    for i, (pred, true) in enumerate(zip(predictions, test_answers)):\n",
    "        if not pred or not true:\n",
    "            result = {\n",
    "                \"correctness_score\": 0.0,\n",
    "                \"semantic_similarity\": 0.0,\n",
    "                \"is_plausible\": 0.0,\n",
    "                \"substring_match\": 0.0,\n",
    "                \"bert_f1_score\": 0.0\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            relevance_metrics.append({\"relevance_score\":0,\"question_similarity\":0})\n",
    "            continue\n",
    "            \n",
    "        pred_norm = normalize_text(pred)\n",
    "        \n",
    "        # Get all the possible answers from value, aliases, normalized values\n",
    "        all_correct_answers = set()\n",
    "        all_correct_answers.add(true['value'])\n",
    "        all_correct_answers.update(true['aliases'])\n",
    "        all_correct_answers.update(true['normalized_aliases'])\n",
    "        all_correct_answers.add(true['normalized_value'])\n",
    "        \n",
    "        # Remove all the empty strings and normalize the correct answers\n",
    "        correct_answers_norm = [normalize_text(ans) for ans in all_correct_answers if ans and str(ans).strip()]\n",
    "        # Remove duplicates and empty\n",
    "        correct_answers_norm = list(set([ans for ans in correct_answers_norm if ans]))\n",
    "        \n",
    "        if not correct_answers_norm:\n",
    "            result = {\n",
    "                \"correctness_score\": 0.0,\n",
    "                \"semantic_similarity\": 0.0,\n",
    "                \"is_plausible\": 0.0,\n",
    "                \"substring_match\": 0.0,\n",
    "                \"bert_f1_score\": 0.0\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            relevance_metrics.append({\"relevance_score\":0,\"question_similarity\":0})\n",
    "            continue\n",
    "        \n",
    "        # Check for substring match\n",
    "        substring_match = any(correct_norm in pred_norm for correct_norm in correct_answers_norm)\n",
    "        \n",
    "        # For semantic metrics use main value as the reference\n",
    "        ref_norm = normalize_text(true['value'])\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\".*RobertaModel.*\")\n",
    "            P, R, F1 = bert_score([pred_norm], [ref_norm], lang='en', verbose=False)\n",
    "            bert_f1_score = float(F1[0])\n",
    "        \n",
    "        semantic_similarity = calculate_semantic_similarity(pred_norm,correct_answers_norm)\n",
    "        if semantic_similarity == 0.0 :\n",
    "            semantic_similarity = calculate_simple_similarity(pred_norm,correct_answers_norm)\n",
    "        \n",
    "        correctness_score = calculate_correctness(substring_match,semantic_similarity,bert_f1_score)\n",
    "        is_plausible = (semantic_similarity > 0.5 or substring_match)\n",
    "        \n",
    "       \n",
    "        #Calculate Answer Relevance\n",
    "        relevance_metric = calculate_answer_relevance(test_questions[i],pred,true['value'])\n",
    "        relevance_metrics.append(relevance_metric)\n",
    "        \n",
    "        text_correctness_results = {\n",
    "            \"correctness_score\": correctness_score,\n",
    "            \"semantic_similarity\": semantic_similarity,\n",
    "            \"is_plausible\": is_plausible,\n",
    "            \"substring_match\": substring_match,\n",
    "            \"bert_f1_score\": bert_f1_score,\n",
    "        }\n",
    "        all_results.append(text_correctness_results)\n",
    "    \n",
    "    # Calculate Average results\n",
    "    aggregated = {\n",
    "        \"total_items\": len(all_results),\n",
    "        \"average_correctness\": sum(r[\"correctness_score\"] for r in all_results) / len(all_results),\n",
    "        \"average_semantic_similarity\": sum(r[\"semantic_similarity\"] for r in all_results) / len(all_results),\n",
    "        \"plausible_count\": sum(r[\"is_plausible\"] for r in all_results),\n",
    "        \"plausible_percentage\": sum(r[\"is_plausible\"] for r in all_results) / len(all_results) * 100,\n",
    "        \"substring_match_count\": sum(r[\"substring_match\"] for r in all_results),\n",
    "        \"bert_f1_score\": sum(r[\"bert_f1_score\"] for r in all_results) / len(all_results),\n",
    "    }\n",
    "    \n",
    "   \n",
    "    # Aggregate relevance metrics\n",
    "    relevance_aggregated = {\n",
    "        \"relevance_score_avg\": sum(r[\"relevance_score\"] for r in relevance_metrics) / len(relevance_metrics),\n",
    "        \"question_similarity_avg\": sum(r[\"question_similarity\"] for r in relevance_metrics) / len(relevance_metrics),\n",
    "    }\n",
    "    \n",
    "    # Compile Results\n",
    "    results = {\n",
    "        \"latency_avg\": np.mean(latencies),\n",
    "        \"latency_std\": np.std(latencies),\n",
    "        \"cpu_time_avg\": np.mean(cpu_times),\n",
    "        \"cpu_time_std\": np.std(cpu_times),\n",
    "        \"memory_usage_avg\": np.mean(memory_usages),\n",
    "        \"average_correctness\": aggregated[\"average_correctness\"],\n",
    "        \"average_semantic_similarity\": aggregated[\"average_semantic_similarity\"],\n",
    "        \"plausible_percentage\": aggregated[\"plausible_percentage\"],\n",
    "        \"substring_match_count\": aggregated[\"substring_match_count\"],\n",
    "        \"bert_f1_score\": aggregated[\"bert_f1_score\"],\n",
    "        \n",
    "        # Relevance metrics\n",
    "        \"answer_relevance_score\": relevance_aggregated[\"relevance_score_avg\"],\n",
    "        \"question_answer_similarity\": relevance_aggregated[\"question_similarity_avg\"],\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead3d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "def setup_logging(results: Dict[str, Any], test_items_count: int, log_dir: str = \"rag_evaluation_logs\"):\n",
    "    \"\"\"Setup logging with timestamp and test item count\"\"\"\n",
    "    \n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    main_folder=\"Logs\"\n",
    "    \n",
    "    # Create filename with timestamp and test count\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"rag_eval_{log_dir}_{timestamp}_{test_items_count}items.json\"\n",
    "    filepath = os.path.join(main_folder, filename)\n",
    "    \n",
    "    # Add metadata to results\n",
    "    results_with_metadata = {\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"test_items_count\": test_items_count,\n",
    "            \"log_file\": filename\n",
    "        },\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_with_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Results logged to: {filepath}\")\n",
    "    return filepath\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87a5f9",
   "metadata": {},
   "source": [
    "### Evaluate RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a4af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation on test set.....\n",
      "Processing question 0/100\n",
      "Processing question 10/100\n",
      "Processing question 20/100\n",
      "Processing question 30/100\n",
      "Processing question 40/100\n",
      "Processing question 50/100\n",
      "Processing question 60/100\n",
      "Processing question 70/100\n",
      "Processing question 80/100\n",
      "Processing question 90/100\n",
      "Results logged to: Logs\\rag_eval_BGERRF_20250924_162854_100items.json\n",
      "\n",
      "=== RAG SYSTEM EVALUATION RESULTS ===\n",
      "Average latency : 11.7983 ± 4.2484 seconds\n",
      "Average CPU time : 45.45 ± 17.06 seconds\n",
      "Average Memory Usage : 34.8039 Mb\n",
      "\n",
      "=== RAG SYSTEM EVALUATION RESULTS FOR QUALITY OF ANSWER ===\n",
      "Average Correctness: 0.6362485070214792\n",
      "Average Semantic Similarity:0.2169619937427342\n",
      "Average Answer Relevance Score: 0.5946941991038621\n",
      "Average Question Answer Similarity: 0.7124152204953134\n",
      "Average Substring Matchcount:33\n",
      "Bert F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "#Create a random test set\n",
    "test_size = 100\n",
    "if len(questions) >= test_size:\n",
    "    indices = random.sample(range(len(questions)), test_size)\n",
    "    \n",
    "    test_questions = [questions[i] for i in indices]\n",
    "    test_answers = [answers[i] for i in indices]\n",
    "\n",
    "print(\"Running Evaluation on test set.....\")\n",
    "results = evaluate_rag_system(test_questions,test_answers,llm_model,tokenizer,k=3)\n",
    "setup_logging(results,len(test_questions),log_dir=\"BGERRF-Tinyllama\")\n",
    "\n",
    "#Print Results\n",
    "print(\"\\n=== RAG SYSTEM EVALUATION RESULTS ===\")\n",
    "print(f\"Average latency : {results['latency_avg']:.4f} ± {results['latency_std']:.4f} seconds\")\n",
    "print(f\"Average CPU time : {results['cpu_time_avg']:.2f} ± {results['cpu_time_std']:.2f} seconds\")\n",
    "print(f\"Average Memory Usage : {results['memory_usage_avg']:.4f} Mb\")\n",
    "\n",
    "print(\"\\n=== RAG SYSTEM EVALUATION RESULTS FOR QUALITY OF ANSWER ===\")\n",
    "\n",
    "print(f\"Average Correctness: {results['average_correctness']}\")\n",
    "print(f\"Average Semantic Similarity:{results['average_semantic_similarity']}\")\n",
    "\n",
    "print(f\"Average Answer Relevance Score: {results['answer_relevance_score']}\")\n",
    "print(f\"Average Question Answer Similarity: {results['question_answer_similarity']}\")\n",
    "\n",
    "print(f\"Average Substring Matchcount:{results['substring_match_count']}\")\n",
    "print(f\"Bert F1 Score: {results['bert_f1_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2745d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation on test set.....\n",
      "Processing question 0/2\n",
      "Number of contexts received: 2\n",
      "First context type: <class 'dict'>\n",
      "First context keys (if dict): dict_keys(['content', 'score'])\n",
      "Number of contexts received: 3\n",
      "First context type: <class 'dict'>\n",
      "First context keys (if dict): dict_keys(['content', 'score'])\n",
      "Results logged to: Logs\\rag_eval_BGERRF_20250924_163616_2items.json\n",
      "\n",
      "=== RAG SYSTEM EVALUATION RESULTS ===\n",
      "Average latency : 13.6652 ± 4.8506 seconds\n",
      "Average CPU time : 56.30 ± 14.90 seconds\n",
      "Average Memory Usage : 0.2480 Mb\n",
      "\n",
      "=== RAG SYSTEM EVALUATION RESULTS FOR QUALITY OF ANSWER ===\n",
      "Average Correctness: 0.4701780085830796\n",
      "Average Semantic Similarity:0.11509966477751732\n",
      "Average Answer Relevance Score: 0.6829853273928165\n",
      "Average Question Answer Similarity: 0.9154719114303589\n",
      "Average Substring Matchcount:0\n",
      "Bert F1 Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "#Create a random test set\n",
    "test_size = 2\n",
    "if len(questions) >= test_size:\n",
    "    indices = random.sample(range(len(questions)), test_size)\n",
    "    \n",
    "    test_questions = [questions[i] for i in indices]\n",
    "    test_answers = [answers[i] for i in indices]\n",
    "\n",
    "print(\"Running Evaluation on test set.....\")\n",
    "results = evaluate_rag_system(test_questions,test_answers,llm_model,tokenizer,k=3)\n",
    "setup_logging(results,len(test_questions),log_dir=\"BGERRF\")\n",
    "\n",
    "#Print Results\n",
    "print(\"\\n=== RAG SYSTEM EVALUATION RESULTS ===\")\n",
    "print(f\"Average latency : {results['latency_avg']:.4f} ± {results['latency_std']:.4f} seconds\")\n",
    "print(f\"Average CPU time : {results['cpu_time_avg']:.2f} ± {results['cpu_time_std']:.2f} seconds\")\n",
    "print(f\"Average Memory Usage : {results['memory_usage_avg']:.4f} Mb\")\n",
    "\n",
    "print(\"\\n=== RAG SYSTEM EVALUATION RESULTS FOR QUALITY OF ANSWER ===\")\n",
    "\n",
    "print(f\"Average Correctness: {results['average_correctness']}\")\n",
    "print(f\"Average Semantic Similarity:{results['average_semantic_similarity']}\")\n",
    "\n",
    "print(f\"Average Answer Relevance Score: {results['answer_relevance_score']}\")\n",
    "print(f\"Average Question Answer Similarity: {results['question_answer_similarity']}\")\n",
    "\n",
    "print(f\"Average Substring Matchcount:{results['substring_match_count']}\")\n",
    "print(f\"Bert F1 Score: {results['bert_f1_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a8515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
