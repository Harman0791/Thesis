{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d46650",
   "metadata": {},
   "source": [
    "### Initial Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c9ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Environment installation complete. running on CPU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    " \n",
    "device='cpu'\n",
    "print(\"Environment installation complete. running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49502d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f45786357604b049d050d86d25be10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4947320b8e4a0eb4b3ee1a94432750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 questions and answers\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#Load the dataset\n",
    "dataset = load_dataset(\"trivia_qa\", \"rc\", split='train[:10000]')\n",
    "\n",
    "#Extract questions and Answers\n",
    "questions = [item['question'] for item in dataset]\n",
    "answers = [item['answer'] for item in dataset]\n",
    "\n",
    "print(f'Loaded {len(questions)} questions and answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f14ab",
   "metadata": {},
   "source": [
    "### Set up Database and Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from evaluate import load\n",
    "import torch\n",
    "#Set pytorch seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#initialize chromadb client\n",
    "chroma_client = chromadb.PersistentClient('./chroma_db_optimized')\n",
    "\n",
    "#embedding function\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "             model_name=\"BAAI/bge-small-en-v1.5\",device='cpu',\n",
    "             trust_remote_code=True    \n",
    "             )\n",
    "\n",
    "#create collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name = 'trivia-qa',\n",
    "    embedding_function= embedding_function,\n",
    "    metadata={\"hnsw:space\":\"cosine\",\n",
    "             \"hnsw:construction_ef\":100,\n",
    "             \"hnsw:search_ef\":50}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121b77b",
   "metadata": {},
   "source": [
    "### Generate Embeddings and populate the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0cfce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate embeddings and populate vector database\n",
      "Processed 0 items\n",
      "Processed 1000 items\n",
      "Processed 2000 items\n",
      "Processed 3000 items\n",
      "Processed 4000 items\n",
      "Processed 5000 items\n",
      "Processed 6000 items\n",
      "Processed 7000 items\n",
      "Processed 8000 items\n",
      "Processed 9000 items\n",
      "Database population complete.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_answer_value(answer_dict):\n",
    "    \"\"\"Extract the primary answer value\"\"\"\n",
    "    if isinstance(answer_dict,dict):\n",
    "        if 'normalized_value' in answer_dict and answer_dict['normalized_value']:\n",
    "            return answer_dict['normalized_value']\n",
    "        if 'value' in answer_dict and answer_dict['value']:\n",
    "            return answer_dict['value']\n",
    "        if 'aliases' in answer_dict and answer_dict['aliases']:\n",
    "            return answer_dict['aliases'][0]\n",
    "    return str(answer_dict)\n",
    "\n",
    "print(\"Generate embeddings and populate vector database\")\n",
    "batch_size = 500 #process in batches to manage memory\n",
    "\n",
    "for i in range(0,len(questions), batch_size):\n",
    "    batch_questions = questions[i:i+batch_size]\n",
    "    batch_answers = answers[i:i+batch_size]\n",
    "    \n",
    "    #convert answers to strings\n",
    "    batch_answer_strings = [extract_answer_value(answer) for answer in batch_answers]\n",
    "    \n",
    "    #generate embeddings for this batch of questions\n",
    "    embeddings = embedding_function(batch_questions)\n",
    "    \n",
    "    #Create IDs for each document\n",
    "    ids = [f\"id_{j}\" for j in range(i,min(i+batch_size,len(questions)))]\n",
    "    \n",
    "    #Add to collection\n",
    "    collection.add(\n",
    "    embeddings= embeddings,\n",
    "    documents= batch_answer_strings,\n",
    "    metadatas= [{\"questions\" : q} for q in batch_questions],\n",
    "    ids=ids\n",
    "    )\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f'Processed {i} items')\n",
    "        \n",
    "print(\"Database population complete.\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df8fc5",
   "metadata": {},
   "source": [
    "### RAG PIPELINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe7bd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Disable gradients for inference\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Initialize models (would be done once, outside the function)\n",
    "# Using a lightweight model for CPU environments\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "class HybridReasoningRetriever:\n",
    "    def __init__(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid retriever with both sparse and dense components.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents to retrieve from\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Initialize sparse retriever (BM25)\n",
    "        self.tokenize = lambda text: re.findall(r'\\w+', text.lower())\n",
    "        tokenized_corpus = [self.tokenize(doc) for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        \n",
    "        # Precompute document embeddings for dense retrieval\n",
    "        self.doc_embeddings = embedding_model.encode(documents, show_progress_bar=False)\n",
    "        \n",
    "        # Initialize query expansion model (would use a lightweight LLM in practice)\n",
    "        self.expansion_model = None  # Placeholder for a small LLM\n",
    "    \n",
    "    def sparse_retrieve(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents using BM25 (sparse retrieval).\n",
    "        \"\"\"\n",
    "        tokenized_query = self.tokenize(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        return [(i, scores[i]) for i in top_indices]\n",
    "    \n",
    "    def dense_retrieve(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents using dense embeddings.\n",
    "        \"\"\"\n",
    "        query_embedding = embedding_model.encode([query])\n",
    "        similarities = np.dot(self.doc_embeddings, query_embedding.T).flatten()\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        return [(i, similarities[i]) for i in top_indices]\n",
    "    \n",
    "\n",
    "    \n",
    "    def hybrid_retrieve(self, query: str, top_k: int = 5, \n",
    "                       alpha: float = 0.7) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Hybrid retrieval combining sparse and dense methods.\n",
    "        \n",
    "        Args:\n",
    "            query: The query to retrieve documents for\n",
    "            top_k: Number of documents to retrieve\n",
    "            alpha: Weight for dense retrieval (1-alpha for sparse)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        retrieval_query = query\n",
    "\n",
    "        \n",
    "        # Get sparse results\n",
    "        sparse_results = self.sparse_retrieve(retrieval_query, top_k * 2)\n",
    "        sparse_scores = np.zeros(len(self.documents))\n",
    "        for idx, score in sparse_results:\n",
    "            sparse_scores[idx] = score\n",
    "        \n",
    "        # Get dense results\n",
    "        dense_results = self.dense_retrieve(retrieval_query, top_k * 2)\n",
    "        dense_scores = np.zeros(len(self.documents))\n",
    "        for idx, score in dense_results:\n",
    "            dense_scores[idx] = score\n",
    "        \n",
    "        # Normalize scores\n",
    "        if np.max(sparse_scores) > 0:\n",
    "            sparse_scores = sparse_scores / np.max(sparse_scores)\n",
    "        if np.max(dense_scores) > 0:\n",
    "            dense_scores = dense_scores / np.max(dense_scores)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = alpha * dense_scores + (1 - alpha) * sparse_scores\n",
    "        top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "        \n",
    "        return [(i, combined_scores[i]) for i in top_indices]\n",
    "\n",
    "def retriever(query: str, k: int = 3, score_threshold: float = 0.4,\n",
    "             max_candidates: int = 30) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Enhanced hybrid retriever with reasoning capabilities.\n",
    "    \n",
    "    Args:\n",
    "        query: Trivia question to search for\n",
    "        k: Number of relevant answers to retrieve\n",
    "        score_threshold: Minimum similarity score to include a result\n",
    "        max_candidates: Maximum number of candidates to consider\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing document content, metadata, and similarity score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        global hybrid_retriever\n",
    "        \n",
    "        # Preprocess query\n",
    "        processed_query = _preprocess_trivia_query(query)\n",
    "        \n",
    "        # Use hybrid retrieval with reasoning capabilities\n",
    "        results = hybrid_retriever.hybrid_retrieve(\n",
    "            processed_query, \n",
    "            top_k=min(k * 5, max_candidates),\n",
    "        )\n",
    "        \n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        # Convert to scored results format\n",
    "        scored_results = []\n",
    "        for idx, score in results:\n",
    "            scored_results.append({\n",
    "                'content': hybrid_retriever.documents[idx],\n",
    "                'score': float(score)  # Convert numpy float to Python float\n",
    "            })\n",
    "        \n",
    "        # Filter by score threshold\n",
    "        filtered_results = [r for r in scored_results if r['score'] >= score_threshold]\n",
    "        \n",
    "        # Filter low-quality context\n",
    "        filtered_results = _filter_low_quality_context(filtered_results)\n",
    "        \n",
    "        # If no results meet threshold, return top results\n",
    "        if not filtered_results and scored_results:\n",
    "            filtered_results = scored_results[:min(3, len(scored_results))]\n",
    "        \n",
    "        # Ensure scores meet minimum threshold\n",
    "        for result in filtered_results:\n",
    "            result['score'] = max(score_threshold - 0.1, result['score'])\n",
    "        \n",
    "        # Sort by score\n",
    "        filtered_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Deduplicate\n",
    "        unique_results = _deduplicate(filtered_results, k)\n",
    "        \n",
    "        return unique_results[:k]\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "# Keep existing helper functions\n",
    "def _preprocess_trivia_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess the query to focus on key entities and facts\n",
    "    \"\"\"\n",
    "    words = query.lower().split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in ['who', 'when', 'where', 'which', 'what']:\n",
    "            filtered_words.append(word)\n",
    "        elif word not in ['is', 'are', 'did', 'do', 'does', 'the', 'a', 'an']:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    return \" \".join(filtered_words) if filtered_words else query\n",
    "\n",
    "def _deduplicate(results: List[Dict], k: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Memory-efficient deduplication for resource-constrained environments\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "    \n",
    "    unique_results = []\n",
    "    seen_content_hashes = set()\n",
    "    \n",
    "    for result in results:\n",
    "        if len(unique_results) >= k * 2:\n",
    "            break\n",
    "        \n",
    "        content = result['content']\n",
    "        content_hash = _simple_content_hash(content)\n",
    "        \n",
    "        if content_hash not in seen_content_hashes:\n",
    "            seen_content_hashes.add(content_hash)\n",
    "            unique_results.append(result)\n",
    "    \n",
    "    return unique_results\n",
    "\n",
    "def _simple_content_hash(content: str, max_length: int = 200) -> int:\n",
    "    \"\"\"\n",
    "    Create a simple hash for content deduplication without heavy processing\n",
    "    \"\"\"\n",
    "    short_content = content[:max_length] if len(content) > max_length else content\n",
    "    return hash(short_content)\n",
    "\n",
    "def _filter_low_quality_context(contexts: List[Dict], min_length: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filter out low-quality context documents\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for ctx in contexts:\n",
    "        content = ctx['content']\n",
    "        \n",
    "        if len(content.strip()) < min_length:\n",
    "            continue\n",
    "            \n",
    "        # Skip JSON-like content\n",
    "        if content.strip().startswith('{') and '}' in content:\n",
    "            continue\n",
    "            \n",
    "        # Skip content with too many special characters\n",
    "        if sum(1 for c in content if not c.isalnum() and not c.isspace()) / len(content) > 0.5:\n",
    "            continue\n",
    "            \n",
    "        filtered.append(ctx)\n",
    "        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33446df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing hybrid retriever with documents...\n",
      "Loaded 10000 documents for hybrid retriever\n",
      "Hybrid retriever initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the hybrid retriever with your actual documents\n",
    "print(\"Initializing hybrid retriever with documents...\")\n",
    "\n",
    "# Extract the actual documents from your collection\n",
    "document_contents = []\n",
    "batch_size = 500\n",
    "\n",
    "for i in range(0, len(questions), batch_size):\n",
    "    batch_end = min(i + batch_size, len(questions))\n",
    "    ids = [f\"id_{j}\" for j in range(i, batch_end)]\n",
    "    \n",
    "    try:\n",
    "        results = collection.get(ids=ids)\n",
    "        document_contents.extend(results['documents'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving batch {i}-{batch_end}: {e}\")\n",
    "        # Fallback: use the original answer strings\n",
    "        document_contents.extend([json.dumps(answer) for answer in answers[i:batch_end]])\n",
    "\n",
    "print(f\"Loaded {len(document_contents)} documents for hybrid retriever\")\n",
    "\n",
    "# Initialize the hybrid retriever (GLOBAL)\n",
    "hybrid_retriever = HybridReasoningRetriever(document_contents)\n",
    "print(\"Hybrid retriever initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee714c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88fc123c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Language Model....\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "print(\"Loading Language Model....\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "#set seed for transformers if available\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "    set_seed(SEED)\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c8135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_rag_prompt(question, context, system_msg=\"You are a factual question-answering assistant.\"):\n",
    "    return f\"\"\"{system_msg}\n",
    "    TRIVIA QUESTION:{question}\n",
    "    \n",
    "    RELEVANT INFORMATION:\n",
    "    {context}\n",
    "    \n",
    "    INSTRUCTION: Based exclusively on the information above, provide a precise factual answer. \n",
    "    Do not generate additional questions or speculative content. \n",
    "    If the answer is not clearly present in the context, state \"Information not found in context.\"\n",
    "    \n",
    "    \n",
    "    FINAL ANSWER:\"\"\"\n",
    "\n",
    "def generator(question,contexts):\n",
    "    \"\"\"\n",
    "    Generate an answer using TinyLlama language model\n",
    "    Uses naive concatenation fusion mechanism\n",
    "    \"\"\"\n",
    "    context_str = \"\\n\".join([f'- {ctx}' for ctx in contexts])\n",
    "    \n",
    "    prompt = create_rag_prompt(question,context_str)\n",
    "    \n",
    "    #Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding = True,truncation=True, max_length=528)\n",
    "    \n",
    "    #Generate the answer\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            inputs.input_ids,\n",
    "            attention_mask = inputs['attention_mask'],\n",
    "            max_new_tokens=30,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.3,\n",
    "            do_sample = True,\n",
    "            pad_token_id = tokenizer.eos_token_id\n",
    "         \n",
    "        )\n",
    "     \n",
    "    #Decode generated text\n",
    "    answer = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    \n",
    "    #Extraxt only new generated part\n",
    "    answer = answer[len(prompt):].strip()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95f08d6",
   "metadata": {},
   "source": [
    "## Functions to Calculate Text Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6751d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "def check_exact_match(predicted: str, true_answers: list) -> bool:\n",
    "    \"\"\"Check for exact match after normalization\"\"\"\n",
    "    pred_norm = normalize_text(predicted)\n",
    "    for true_answer in true_answers:\n",
    "        true_norm = normalize_text(true_answer)\n",
    "        if pred_norm == true_norm:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def calculate_semantic_similarity(text1:str, text2:str)-> float:\n",
    "    if not text1 or not text2:\n",
    "        return 0.0\n",
    "    \n",
    "    if model is not None:\n",
    "        try:\n",
    "            embeddings1 = model.encode(text1,convert_to_tensor=True)\n",
    "            embeddings2 = model.encode(text2,convert_to_tensor=True)\n",
    "            cosine_scores = util.pytorch_cos_sim(embeddings1,embeddings2)\n",
    "            return float(cosine_scores[0][0])\n",
    "        except Exception as e:\n",
    "            print(f\"Semantic similarity calculation failed : {e}\")\n",
    "\n",
    "def calculate_simple_similarity(text1, text2) -> float:\n",
    "    # Convert to strings if they are lists\n",
    "    if isinstance(text1, list):\n",
    "        text1 = ' '.join(text1)\n",
    "    if isinstance(text2, list):\n",
    "        text2 = ' '.join(text2)\n",
    "    \n",
    "    words1 = set(text1.split())\n",
    "    words2 = set(text2.split())\n",
    "    \n",
    "    if not words1 or not words2:\n",
    "        return 0.0\n",
    "    return len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "def calculate_correctness(substring_match,semantic_similarity,bert_f1_score):\n",
    "    substring_match_bool = bool(substring_match)\n",
    "    semantic_sim_norm = max(0.0,min(1.0,semantic_similarity))\n",
    "    bert_f1_norm = max(0.0,min(1.0,bert_f1_score))\n",
    "    \n",
    "    if substring_match_bool:\n",
    "        base_score = 0.80\n",
    "        bonus = 0.20 * (0.5 * semantic_sim_norm + 0.5 * bert_f1_norm)\n",
    "        return min(1.0,base_score + bonus)\n",
    "    else:\n",
    "        semantic_weight = 0.6\n",
    "        bert_f1_weight = 0.4\n",
    "        composite_score = (semantic_weight * semantic_sim_norm + bert_f1_weight * bert_f1_norm)\n",
    "        final_score = composite_score ** 0.8\n",
    "        return final_score\n",
    "    \n",
    "def normalize_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Remove extra whitespace and punctuation\n",
    "    text = re.sub(r'[^\\\\w\\\\s]', ' ', text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text)\n",
    "    \n",
    "    # Remove common stopwords for better matching\n",
    "    stopwords = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'and', 'or', 'but'}\n",
    "    words = [word for word in text.split() if word not in stopwords and len(word) > 1]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def calculate_answer_relevance(question: str, generated_answer: str, \n",
    "                             ground_truth_answer: str = None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate how relevant the answer is to the question\n",
    "    \"\"\"\n",
    "    if not question or not generated_answer:\n",
    "        return {\"relevance_score\": 0.0, \"question_similarity\": 0.0}\n",
    "    \n",
    "    # Method 1: Direct similarity between question and answer\n",
    "    question_answer_similarity = calculate_semantic_similarity(question, generated_answer)\n",
    "    \n",
    "    # Method 2: If ground truth is available, compare answer similarity patterns\n",
    "    if ground_truth_answer:\n",
    "        gt_question_similarity = calculate_semantic_similarity(question, ground_truth_answer)\n",
    "        answer_gt_similarity = calculate_semantic_similarity(generated_answer, ground_truth_answer)\n",
    "        \n",
    "        # Relevance score combines direct similarity and alignment with ground truth pattern\n",
    "        relevance_score = 0.7 * question_answer_similarity + 0.3 * answer_gt_similarity\n",
    "    else:\n",
    "        relevance_score = question_answer_similarity\n",
    "    \n",
    "    return {\n",
    "        \"relevance_score\": min(1.0, relevance_score),\n",
    "        \"question_similarity\": question_answer_similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "884d4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from bert_score import score as bert_score\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import torch\n",
    "\n",
    "# Suppress all transformers warnings\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# More comprehensive warning suppression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"bert_score\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "\n",
    "def evaluate_rag_system(test_questions, test_answers, llm_model, tokenizer, k=3):\n",
    "    \"\"\"\n",
    "    test_questions (list[str]): Input questions\n",
    "    test_answers (list[str]): Ground truth answers\n",
    "    llm_model: Preloaded language model (CPU)\n",
    "    tokenizer: Preloaded tokenizer\n",
    "    k (int): Top-k retrieved documents\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    latencies = []\n",
    "    cpu_times = []\n",
    "    memory_usages = []\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    for i, (question, true_answer) in enumerate(zip(test_questions, test_answers)):\n",
    "        if i % 10 == 0:\n",
    "            print(f'Processing question {i}/{len(test_questions)}')\n",
    "            \n",
    "        # Start time\n",
    "        start_time = time.time()\n",
    "        cpu_start = process.cpu_times()\n",
    "        mem_start = process.memory_info().rss\n",
    "        \n",
    "        # Retrieve Context and generate answer\n",
    "        contexts_dicts = retriever(question, k=k)\n",
    "        \n",
    "        # Generate Answers\n",
    "        answer = generator(question, contexts_dicts)\n",
    "        \n",
    "        # End timing and calculate metrics\n",
    "        end_time = time.time()\n",
    "        cpu_end = process.cpu_times()\n",
    "        mem_end = process.memory_info().rss\n",
    "        \n",
    "        latency = end_time - start_time\n",
    "        cpu_time = (cpu_end.user - cpu_start.user) + (cpu_end.system - cpu_start.system)\n",
    "        memory_usage = (mem_end - mem_start) / 1024 / 1024  # Convert to MB\n",
    "        \n",
    "        predictions.append(answer)\n",
    "        latencies.append(latency)\n",
    "        cpu_times.append(cpu_time)\n",
    "        memory_usages.append(max(0, memory_usage))\n",
    "\n",
    "    all_results = []\n",
    "    relevance_metrics=[]\n",
    "    faithfulness_metrics =[]\n",
    "    \n",
    "    for i, (pred, true) in enumerate(zip(predictions, test_answers)):\n",
    "        if not pred or not true:\n",
    "            result = {\n",
    "                \"correctness_score\": 0.0,\n",
    "                \"semantic_similarity\": 0.0,\n",
    "                \"is_plausible\": 0.0,\n",
    "                \"substring_match\": 0.0,\n",
    "                \"bert_f1_score\": 0.0\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            relevance_metrics.append({\"relevance_score\":0,\"question_similarity\":0})\n",
    "            continue\n",
    "            \n",
    "        pred_norm = normalize_text(pred)\n",
    "        \n",
    "        # Get all the possible answers from value, aliases, normalized values\n",
    "        all_correct_answers = set()\n",
    "        all_correct_answers.add(true['value'])\n",
    "        all_correct_answers.update(true['aliases'])\n",
    "        all_correct_answers.update(true['normalized_aliases'])\n",
    "        all_correct_answers.add(true['normalized_value'])\n",
    "        \n",
    "        # Remove all the empty strings and normalize the correct answers\n",
    "        correct_answers_norm = [normalize_text(ans) for ans in all_correct_answers if ans and str(ans).strip()]\n",
    "        # Remove duplicates and empty\n",
    "        correct_answers_norm = list(set([ans for ans in correct_answers_norm if ans]))\n",
    "        \n",
    "        if not correct_answers_norm:\n",
    "            result = {\n",
    "                \"correctness_score\": 0.0,\n",
    "                \"semantic_similarity\": 0.0,\n",
    "                \"is_plausible\": 0.0,\n",
    "                \"substring_match\": 0.0,\n",
    "                \"bert_f1_score\": 0.0\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            relevance_metrics.append({\"relevance_score\":0,\"question_similarity\":0})\n",
    "            continue\n",
    "        \n",
    "        # Check for substring match\n",
    "        substring_match = any(correct_norm in pred_norm for correct_norm in correct_answers_norm)\n",
    "        exact_match = check_exact_match(pred_norm, correct_answers_norm)\n",
    "        \n",
    "        # For semantic metrics use main value as the reference\n",
    "        ref_norm = normalize_text(true['value'])\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\".*RobertaModel.*\")\n",
    "            P, R, F1 = bert_score([pred_norm], [ref_norm], lang='en', verbose=False)\n",
    "            bert_f1_score = float(F1[0])\n",
    "        \n",
    "        semantic_similarity = calculate_semantic_similarity(pred_norm,correct_answers_norm)\n",
    "        if semantic_similarity == 0.0 :\n",
    "            semantic_similarity = calculate_simple_similarity(pred_norm,correct_answers_norm)\n",
    "        \n",
    "        correctness_score = calculate_correctness(substring_match or exact_match,\n",
    "                                                  semantic_similarity,bert_f1_score)\n",
    "        is_plausible = (semantic_similarity > 0.5 or substring_match)\n",
    "\n",
    "        \n",
    "        #Calculate Answer Relevance\n",
    "        relevance_metric = calculate_answer_relevance(test_questions[i],pred,true['value'])\n",
    "        relevance_metrics.append(relevance_metric)\n",
    "        \n",
    "        text_correctness_results = {\n",
    "            \"correctness_score\": correctness_score,\n",
    "            \"semantic_similarity\": semantic_similarity,\n",
    "            \"is_plausible\": is_plausible,\n",
    "            \"substring_match\": substring_match,\n",
    "            \"bert_f1_score\": bert_f1_score,\n",
    "        }\n",
    "        all_results.append(text_correctness_results)\n",
    "    \n",
    "    # Calculate Average results\n",
    "    aggregated = {\n",
    "        \"total_items\": len(all_results),\n",
    "        \"average_correctness\": sum(r[\"correctness_score\"] for r in all_results) / len(all_results),\n",
    "        \"average_semantic_similarity\": sum(r[\"semantic_similarity\"] for r in all_results) / len(all_results),\n",
    "        \"plausible_count\": sum(r[\"is_plausible\"] for r in all_results),\n",
    "        \"plausible_percentage\": sum(r[\"is_plausible\"] for r in all_results) / len(all_results) * 100,\n",
    "        \"substring_match_count\": sum(r[\"substring_match\"] for r in all_results),\n",
    "        \"bert_f1_score\": sum(r[\"bert_f1_score\"] for r in all_results) / len(all_results),\n",
    "    }\n",
    "    \n",
    "   \n",
    "    # Aggregate relevance metrics\n",
    "    relevance_aggregated = {\n",
    "        \"relevance_score_avg\": sum(r[\"relevance_score\"] for r in relevance_metrics) / len(relevance_metrics),\n",
    "        \"question_similarity_avg\": sum(r[\"question_similarity\"] for r in relevance_metrics) / len(relevance_metrics),\n",
    "    }\n",
    "    \n",
    "    # Compile Results\n",
    "    results = {\n",
    "        \"latency_avg\": np.mean(latencies),\n",
    "        \"latency_std\": np.std(latencies),\n",
    "        \"cpu_time_avg\": np.mean(cpu_times),\n",
    "        \"cpu_time_std\": np.std(cpu_times),\n",
    "        \"memory_usage_avg\": np.mean(memory_usages),\n",
    "        \"average_correctness\": aggregated[\"average_correctness\"],\n",
    "        \"average_semantic_similarity\": aggregated[\"average_semantic_similarity\"],\n",
    "        \"plausible_percentage\": aggregated[\"plausible_percentage\"],\n",
    "        \"substring_match_count\": aggregated[\"substring_match_count\"],\n",
    "        \"bert_f1_score\": aggregated[\"bert_f1_score\"],\n",
    "        \n",
    "        # Relevance metrics\n",
    "        \"answer_relevance_score\": relevance_aggregated[\"relevance_score_avg\"],\n",
    "        \"question_answer_similarity\": relevance_aggregated[\"question_similarity_avg\"],\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd65320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "def setup_logging(results: Dict[str, Any], test_items_count: int, log_dir: str = \"rag_evaluation_logs\"):\n",
    "    \"\"\"Setup logging with timestamp and test item count\"\"\"\n",
    "    \n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    main_folder=\"Logs\"\n",
    "    \n",
    "    # Create filename with timestamp and test count\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"rag_eval_{log_dir}_{timestamp}_{test_items_count}items.json\"\n",
    "    filepath = os.path.join(main_folder, filename)\n",
    "    \n",
    "    # Add metadata to results\n",
    "    results_with_metadata = {\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"test_items_count\": test_items_count,\n",
    "            \"log_file\": filename\n",
    "        },\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_with_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Results logged to: {filepath}\")\n",
    "    return filepath\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87a5f9",
   "metadata": {},
   "source": [
    "### Evaluate RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cfee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation on test set.....\n",
      "Processing question 0/100\n",
      "Processing question 10/100\n",
      "Processing question 20/100\n",
      "Processing question 30/100\n",
      "Processing question 40/100\n",
      "Processing question 50/100\n",
      "Processing question 60/100\n",
      "Processing question 70/100\n",
      "Processing question 80/100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "#Create a random test set\n",
    "test_size = 100\n",
    "if len(questions) >= test_size:\n",
    "    indices = random.sample(range(len(questions)), test_size)\n",
    "    \n",
    "    test_questions = [questions[i] for i in indices]\n",
    "    test_answers = [answers[i] for i in indices]\n",
    "\n",
    "print(\"Running Evaluation on test set.....\")\n",
    "results = evaluate_rag_system(test_questions,test_answers,llm_model,tokenizer,k=3)\n",
    "setup_logging(results,len(test_questions),log_dir=\"L12Naive\")\n",
    "\n",
    "#Print Results\n",
    "print(\"\\n=== RAG SYSTEM EVALUATION RESULTS ===\")\n",
    "print(f\"Average latency : {results['latency_avg']:.4f} ± {results['latency_std']:.4f} seconds\")\n",
    "print(f\"Average CPU time : {results['cpu_time_avg']:.2f} ± {results['cpu_time_std']:.2f} seconds\")\n",
    "print(f\"Average Memory Usage : {results['memory_usage_avg']:.4f} Mb\")\n",
    "\n",
    "print(\"\\n=== RAG SYSTEM EVALUATION RESULTS FOR QUALITY OF ANSWER ===\")\n",
    "\n",
    "print(f\"Average Correctness: {results['average_correctness']}\")\n",
    "print(f\"Average Semantic Similarity:{results['average_semantic_similarity']}\")\n",
    "\n",
    "print(f\"Average Answer Relevance Score: {results['answer_relevance_score']}\")\n",
    "print(f\"Average Question Answer Similarity: {results['question_answer_similarity']}\")\n",
    "\n",
    "print(f\"Average Substring Matchcount:{results['substring_match_count']}\")\n",
    "print(f\"Bert F1 Score: {results['bert_f1_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6972651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
