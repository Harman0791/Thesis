{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d46650",
   "metadata": {},
   "source": [
    "### Initial Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c9ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Environment installation complete. running on CPU\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*RobertaModel.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pooler.dense.*\")\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    " \n",
    "device='cpu'\n",
    "print(\"Environment installation complete. running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49502d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbacd2f8877046b3a6722404db9ed7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c90aa22cea14b6da84a133d15d5ecdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 questions and answers\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#Load the dataset\n",
    "dataset = load_dataset(\"trivia_qa\", \"rc\", split='train[:10000]')\n",
    "\n",
    "#Extract questions and Answers\n",
    "questions = [item['question'] for item in dataset]\n",
    "answers = [item['answer'] for item in dataset]\n",
    "\n",
    "print(f'Loaded {len(questions)} questions and answers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f14ab",
   "metadata": {},
   "source": [
    "### Set up Database and Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cabca9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from evaluate import load\n",
    "import torch\n",
    "#Set pytorch seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "#initialize chromadb client\n",
    "chroma_client = chromadb.PersistentClient('./chroma_db_optimized')\n",
    "\n",
    "#embedding function\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "             model_name=\"BAAI/bge-small-en-v1.5\",device='cpu',\n",
    "             trust_remote_code=True    \n",
    "             )\n",
    "#create collection\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name = 'trivia-qa',\n",
    "    embedding_function= embedding_function,\n",
    "    metadata={\"hnsw:space\":\"cosine\",\n",
    "             \"hnsw:construction_ef\":100,\n",
    "             \"hnsw:search_ef\":50}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da27049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Language Model....\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "#set seed for transformers if available\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "    set_seed(SEED)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "print(\"Loading Language Model....\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df998d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d121b77b",
   "metadata": {},
   "source": [
    "### Generate Embeddings and populate the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cfce90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate embeddings and populate vector database\n",
      "Processed 0 items\n",
      "Processed 1000 items\n",
      "Processed 2000 items\n",
      "Processed 3000 items\n",
      "Processed 4000 items\n",
      "Processed 5000 items\n",
      "Processed 6000 items\n",
      "Processed 7000 items\n",
      "Processed 8000 items\n",
      "Processed 9000 items\n",
      "Database population complete.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(\"Generate embeddings and populate vector database\")\n",
    "batch_size = 500 #process in batches to manage memory\n",
    "\n",
    "for i in range(0,len(questions), batch_size):\n",
    "    batch_questions = questions[i:i+batch_size]\n",
    "    batch_answers = answers[i:i+batch_size]\n",
    "    \n",
    "    #convert answers to strings\n",
    "    batch_answer_strings = [json.dumps(answer) for answer in batch_answers]\n",
    "    \n",
    "    #generate embeddings for this batch of questions\n",
    "    embeddings = embedding_function(batch_questions)\n",
    "    \n",
    "    #Create IDs for each document\n",
    "    ids = [f\"id_{j}\" for j in range(i,min(i+batch_size,len(questions)))]\n",
    "    \n",
    "    #Add to collection\n",
    "    collection.add(\n",
    "    embeddings= embeddings,\n",
    "    documents= batch_answer_strings,\n",
    "    metadatas= [{\"questions\" : q} for q in batch_questions],\n",
    "    ids=ids\n",
    "    )\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f'Processed {i} items')\n",
    "        \n",
    "print(\"Database population complete.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19f75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8df8fc5",
   "metadata": {},
   "source": [
    "### RAG PIPELINE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7bd647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import logging\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Disable gradients for inference\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Initialize models (would be done once, outside the function)\n",
    "# Using a lightweight model for CPU environments\n",
    "embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "\n",
    "class HybridReasoningRetriever:\n",
    "    def __init__(self, documents: List[str]):\n",
    "        \"\"\"\n",
    "        Initialize the hybrid retriever with both sparse and dense components.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of text documents to retrieve from\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        \n",
    "        # Initialize sparse retriever (BM25)\n",
    "        self.tokenize = lambda text: re.findall(r'\\w+', text.lower())\n",
    "        tokenized_corpus = [self.tokenize(doc) for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_corpus)\n",
    "        \n",
    "        # Precompute document embeddings for dense retrieval\n",
    "        self.doc_embeddings = embedding_model.encode(documents, show_progress_bar=False)\n",
    "        \n",
    "        # Initialize query expansion model (would use a lightweight LLM in practice)\n",
    "        self.expansion_model = None  # Placeholder for a small LLM\n",
    "    \n",
    "    def sparse_retrieve(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents using BM25 (sparse retrieval).\n",
    "        \"\"\"\n",
    "        tokenized_query = self.tokenize(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        return [(i, scores[i]) for i in top_indices]\n",
    "    \n",
    "    def dense_retrieve(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents using dense embeddings.\n",
    "        \"\"\"\n",
    "        query_embedding = embedding_model.encode([query])\n",
    "        similarities = np.dot(self.doc_embeddings, query_embedding.T).flatten()\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        return [(i, similarities[i]) for i in top_indices]\n",
    "    \n",
    "\n",
    "    \n",
    "    def hybrid_retrieve(self, query: str, top_k: int = 5, \n",
    "                       alpha: float = 0.7) -> List[Tuple[int, float]]:\n",
    "        \"\"\"\n",
    "        Hybrid retrieval combining sparse and dense methods.\n",
    "        \n",
    "        Args:\n",
    "            query: The query to retrieve documents for\n",
    "            top_k: Number of documents to retrieve\n",
    "            alpha: Weight for dense retrieval (1-alpha for sparse)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        retrieval_query = query\n",
    "\n",
    "        \n",
    "        # Get sparse results\n",
    "        sparse_results = self.sparse_retrieve(retrieval_query, top_k * 2)\n",
    "        sparse_scores = np.zeros(len(self.documents))\n",
    "        for idx, score in sparse_results:\n",
    "            sparse_scores[idx] = score\n",
    "        \n",
    "        # Get dense results\n",
    "        dense_results = self.dense_retrieve(retrieval_query, top_k * 2)\n",
    "        dense_scores = np.zeros(len(self.documents))\n",
    "        for idx, score in dense_results:\n",
    "            dense_scores[idx] = score\n",
    "        \n",
    "        # Normalize scores\n",
    "        if np.max(sparse_scores) > 0:\n",
    "            sparse_scores = sparse_scores / np.max(sparse_scores)\n",
    "        if np.max(dense_scores) > 0:\n",
    "            dense_scores = dense_scores / np.max(dense_scores)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined_scores = alpha * dense_scores + (1 - alpha) * sparse_scores\n",
    "        top_indices = np.argsort(combined_scores)[::-1][:top_k]\n",
    "        \n",
    "        return [(i, combined_scores[i]) for i in top_indices]\n",
    "\n",
    "def retriever(query: str, k: int = 3, score_threshold: float = 0.4,\n",
    "             max_candidates: int = 30) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Enhanced hybrid retriever with reasoning capabilities.\n",
    "    \n",
    "    Args:\n",
    "        query: Trivia question to search for\n",
    "        k: Number of relevant answers to retrieve\n",
    "        score_threshold: Minimum similarity score to include a result\n",
    "        max_candidates: Maximum number of candidates to consider\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing document content, metadata, and similarity score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        global hybrid_retriever\n",
    "        \n",
    "        # Preprocess query\n",
    "        processed_query = _preprocess_trivia_query(query)\n",
    "        \n",
    "        # Use hybrid retrieval with reasoning capabilities\n",
    "        results = hybrid_retriever.hybrid_retrieve(\n",
    "            processed_query, \n",
    "            top_k=min(k * 5, max_candidates),\n",
    "        )\n",
    "        \n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        # Convert to scored results format\n",
    "        scored_results = []\n",
    "        for idx, score in results:\n",
    "            scored_results.append({\n",
    "                'content': hybrid_retriever.documents[idx],\n",
    "                'score': float(score)  # Convert numpy float to Python float\n",
    "            })\n",
    "        \n",
    "        # Filter by score threshold\n",
    "        filtered_results = [r for r in scored_results if r['score'] >= score_threshold]\n",
    "        \n",
    "        # Filter low-quality context\n",
    "        filtered_results = _filter_low_quality_context(filtered_results)\n",
    "        \n",
    "        # If no results meet threshold, return top results\n",
    "        if not filtered_results and scored_results:\n",
    "            filtered_results = scored_results[:min(3, len(scored_results))]\n",
    "        \n",
    "        # Ensure scores meet minimum threshold\n",
    "        for result in filtered_results:\n",
    "            result['score'] = max(score_threshold - 0.1, result['score'])\n",
    "        \n",
    "        # Sort by score\n",
    "        filtered_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Deduplicate\n",
    "        unique_results = _deduplicate(filtered_results, k)\n",
    "        \n",
    "        return unique_results[:k]\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in retrieval: {e}\")\n",
    "        return []\n",
    "\n",
    "# Keep existing helper functions\n",
    "def _preprocess_trivia_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess the query to focus on key entities and facts\n",
    "    \"\"\"\n",
    "    words = query.lower().split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in ['who', 'when', 'where', 'which', 'what']:\n",
    "            filtered_words.append(word)\n",
    "        elif word not in ['is', 'are', 'did', 'do', 'does', 'the', 'a', 'an']:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    return \" \".join(filtered_words) if filtered_words else query\n",
    "\n",
    "def _deduplicate(results: List[Dict], k: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Memory-efficient deduplication for resource-constrained environments\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        return []\n",
    "    \n",
    "    unique_results = []\n",
    "    seen_content_hashes = set()\n",
    "    \n",
    "    for result in results:\n",
    "        if len(unique_results) >= k * 2:\n",
    "            break\n",
    "        \n",
    "        content = result['content']\n",
    "        content_hash = _simple_content_hash(content)\n",
    "        \n",
    "        if content_hash not in seen_content_hashes:\n",
    "            seen_content_hashes.add(content_hash)\n",
    "            unique_results.append(result)\n",
    "    \n",
    "    return unique_results\n",
    "\n",
    "def _simple_content_hash(content: str, max_length: int = 200) -> int:\n",
    "    \"\"\"\n",
    "    Create a simple hash for content deduplication without heavy processing\n",
    "    \"\"\"\n",
    "    short_content = content[:max_length] if len(content) > max_length else content\n",
    "    return hash(short_content)\n",
    "\n",
    "def _filter_low_quality_context(contexts: List[Dict], min_length: int = 10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Filter out low-quality context documents\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for ctx in contexts:\n",
    "        content = ctx['content']\n",
    "        \n",
    "        if len(content.strip()) < min_length:\n",
    "            continue\n",
    "            \n",
    "        # Skip JSON-like content\n",
    "        if content.strip().startswith('{') and '}' in content:\n",
    "            continue\n",
    "            \n",
    "        # Skip content with too many special characters\n",
    "        if sum(1 for c in content if not c.isalnum() and not c.isspace()) / len(content) > 0.5:\n",
    "            continue\n",
    "            \n",
    "        filtered.append(ctx)\n",
    "        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f64d39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing hybrid retriever with documents...\n",
      "Loaded 10000 documents for hybrid retriever\n",
      "Hybrid retriever initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the hybrid retriever with your actual documents\n",
    "print(\"Initializing hybrid retriever with documents...\")\n",
    "\n",
    "# Extract the actual documents from your collection\n",
    "document_contents = []\n",
    "batch_size = 500\n",
    "\n",
    "for i in range(0, len(questions), batch_size):\n",
    "    batch_end = min(i + batch_size, len(questions))\n",
    "    ids = [f\"id_{j}\" for j in range(i, batch_end)]\n",
    "    \n",
    "    try:\n",
    "        results = collection.get(ids=ids)\n",
    "        document_contents.extend(results['documents'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving batch {i}-{batch_end}: {e}\")\n",
    "        # Fallback: use the original answer strings\n",
    "        document_contents.extend([json.dumps(answer) for answer in answers[i:batch_end]])\n",
    "\n",
    "print(f\"Loaded {len(document_contents)} documents for hybrid retriever\")\n",
    "\n",
    "# Initialize the hybrid retriever (GLOBAL)\n",
    "hybrid_retriever = HybridReasoningRetriever(document_contents)\n",
    "print(\"Hybrid retriever initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c8135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import math\n",
    "import json\n",
    "\n",
    "def create_rag_prompt(question, context, system_msg=\"You are a factual question-answering assistant.\"):\n",
    "    return f\"\"\"{system_msg}\n",
    "    TRIVIA QUESTION:{question}\n",
    "    \n",
    "    RELEVANT INFORMATION:\n",
    "    {context}\n",
    "    \n",
    "    INSTRUCTION: Based exclusively on the information above, provide a precise factual answer. \n",
    "    Do not generate additional questions or speculative content. \n",
    "    If the answer is not clearly present in the context, state \"Information not found in context.\"\n",
    "    \n",
    "    \n",
    "    FINAL ANSWER:\"\"\"\n",
    "\n",
    "def doc_contains_token(doc,token):\n",
    "    token = token.lower()\n",
    "    if isinstance(doc,list):\n",
    "        for t in doc:\n",
    "            if token in str(t).lower():\n",
    "                return True\n",
    "            return False\n",
    "        else:\n",
    "            return token in str(doc).lower()\n",
    "        \n",
    "\n",
    "def generator(question,contexts,llm_model,tokenizer):\n",
    "    \"\"\"\n",
    "    Generate an answer using language model\n",
    "    optimized for cpu execution with lightweight BM25-inspired heuristic reranking\n",
    "    \n",
    "    Args:\n",
    "    question(str) : question to answer\n",
    "    contexts(list[str]) : A list of context passages from vector db retrieval\n",
    "    llm_model : pre-loaded language model\n",
    "    tokenizer : The pre-loaded tokenizer for the model\n",
    "    \"\"\"\n",
    "    \n",
    "    context_texts =[]\n",
    "\n",
    "    for ctx in contexts:\n",
    "        content = json.loads(ctx[\"content\"])\n",
    "        aliases = content.get('aliases',[])\n",
    "        normalized_aliases = content.get('normalized_aliases',[])\n",
    "        value = content.get('value',[])\n",
    "        \n",
    "            \n",
    "        context_str = []\n",
    "        \n",
    "        \n",
    "        if aliases:\n",
    "            context_str.append(f\"Aliases : {', '.join(aliases[:5])}\")\n",
    "        if normalized_aliases:\n",
    "            context_str.append(f\"Normalized : {', '.join(normalized_aliases[:3])}\")\n",
    "            \n",
    "        context_texts.append(context_str)\n",
    "        \n",
    "            \n",
    "        \n",
    "  \n",
    "    if len(context_texts) > 1:\n",
    "       \n",
    "        question_tokens = set(re.findall(r'\\w+', question.lower()))\n",
    "        \n",
    "        #Calculate scores using BM25-inspired heuristic\n",
    "        scored_contexts = []\n",
    "        context_strings = [str(ctx) for ctx in context_texts]\n",
    "        avg_length = sum(len(ctx.split()) for ctx in context_strings) / len(context_strings)\n",
    "        \n",
    "        for i, ctx in enumerate(context_texts):\n",
    "            ctx_str = str(ctx)\n",
    "            ctx_tokens = re.findall(r'\\w+', ctx_str.lower())\n",
    "            ctx_token_set = set(ctx_tokens)\n",
    "            \n",
    "            score=0\n",
    "            for token in question_tokens:\n",
    "                if token in ctx_token_set:\n",
    "                    #simple term frequency\n",
    "                    tf = ctx_tokens.count(token)   \n",
    "                    \n",
    "                    #Approximate IDF \n",
    "                    token_l = token.lower()\n",
    "                    doc_freq = sum(1 for c in context_texts if doc_contains_token(c,token_l))\n",
    "                    N = len(context_texts)\n",
    "                    idf = math.log((N -doc_freq +0.5) / (doc_freq + 0.5) + 1)\n",
    "                    \n",
    "                    #BM25-like scoring with length normalization\n",
    "                    k1 = 1.2\n",
    "                    b = 0.75\n",
    "                    length_ratio = len(ctx_tokens)/ avg_length\n",
    "                    norm_tf =  (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * length_ratio))\n",
    "                    \n",
    "                    score += norm_tf * idf\n",
    "            \n",
    "            scored_contexts.append((ctx,score,i))\n",
    "        #Sort by score descending\n",
    "        scored_contexts.sort(key=lambda x:x[1], reverse=True)\n",
    "        contexts = [ctx for ctx,score,idx in scored_contexts]\n",
    "        \n",
    "    \n",
    "    #Prompt construction with fused context\n",
    "    system_msg = \"Answer based on this information:\"\n",
    "    base_prompt = f\"{system_msg}\\nQuestion: {question}\\nInfo:\\n\"\n",
    "    \n",
    "    #Estimate token count without full tokenization to save CPU\n",
    "    base_token_estimate = len(base_prompt.split()) * 1.3\n",
    "    max_token_counts = 700 - int(base_token_estimate)\n",
    "    \n",
    "    effective_context = \"\"\n",
    "    current_token_estimate = 0\n",
    "    context_used = 0\n",
    "    \n",
    "    for ctx in contexts:\n",
    "        ctx_str = str(ctx)\n",
    "        clean_ctx = ' '.join(ctx_str.split()[:100])  # Limit context length upfront\n",
    "        ctx_token_estimate = len(clean_ctx.split()) * 1.3\n",
    "        \n",
    "        if current_token_estimate + ctx_token_estimate <= max_token_counts:\n",
    "            effective_context += f\"- {clean_ctx}\\n\"\n",
    "            current_token_estimate += ctx_token_estimate\n",
    "            context_used += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    final_prompt = create_rag_prompt(question,effective_context)\n",
    "    #final_prompt = base_prompt + effective_context + \"\\nAnswer:\"\n",
    "    \n",
    "    #CPU tokenization\n",
    "    inputs = tokenizer(\n",
    "        final_prompt,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        max_length=528\n",
    "    )\n",
    "    \n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=30,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.3,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,    \n",
    "    )\n",
    "    \n",
    "    llm_model.eval()\n",
    "        \n",
    "    #Generate the answer\n",
    "    with torch.inference_mode():\n",
    "        outputs = llm_model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config\n",
    "        )\n",
    "    \n",
    "    generated_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    answer = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "\n",
    "            \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831ef29",
   "metadata": {},
   "source": [
    "## Functions to Calculate Text Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c466ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "def calculate_semantic_similarity(text1:str, text2:str)-> float:\n",
    "    if not text1 or not text2:\n",
    "        return 0.0\n",
    "    \n",
    "    if model is not None:\n",
    "        try:\n",
    "            embeddings1 = model.encode(text1,convert_to_tensor=True)\n",
    "            embeddings2 = model.encode(text2,convert_to_tensor=True)\n",
    "            cosine_scores = util.pytorch_cos_sim(embeddings1,embeddings2)\n",
    "            return float(cosine_scores[0][0])\n",
    "        except Exception as e:\n",
    "            print(f\"Semantic similarity calculation failed : {e}\")\n",
    "\n",
    "def calculate_simple_similarity(text1:str, text2:str)-> float:\n",
    "    words1 = set(text1.split())\n",
    "    words2 = set(text2.split())\n",
    "    \n",
    "    if not words1 or not words2:\n",
    "        return 0.0\n",
    "    return len(words1.intersection(words2)) / len(words1.union(words2))\n",
    "\n",
    "def calculate_correctness(substring_match,semantic_similarity,bert_f1_score):\n",
    "    substring_match_bool = bool(substring_match)\n",
    "    semantic_sim_norm = max(0.0,min(1.0,semantic_similarity))\n",
    "    bert_f1_norm = max(0.0,min(1.0,bert_f1_score))\n",
    "    \n",
    "    if substring_match_bool:\n",
    "        base_score = 0.80\n",
    "        bonus = 0.20 * (0.5 * semantic_sim_norm + 0.5 * bert_f1_norm)\n",
    "        return min(1.0,base_score + bonus)\n",
    "    else:\n",
    "        semantic_weight = 0.6\n",
    "        bert_f1_weight = 0.4\n",
    "        composite_score = (semantic_weight * semantic_sim_norm + bert_f1_weight * bert_f1_norm)\n",
    "        final_score = composite_score ** 0.8\n",
    "        return final_score\n",
    "    \n",
    "def normalize_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def calculate_answer_relevance(question: str, generated_answer: str, \n",
    "                             ground_truth_answer: str = None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate how relevant the answer is to the question\n",
    "    \"\"\"\n",
    "    if not question or not generated_answer:\n",
    "        return {\"relevance_score\": 0.0, \"question_similarity\": 0.0}\n",
    "    \n",
    "    # Method 1: Direct similarity between question and answer\n",
    "    question_answer_similarity = calculate_semantic_similarity(question, generated_answer)\n",
    "    \n",
    "    # Method 2: If ground truth is available, compare answer similarity patterns\n",
    "    if ground_truth_answer:\n",
    "        gt_question_similarity = calculate_semantic_similarity(question, ground_truth_answer)\n",
    "        answer_gt_similarity = calculate_semantic_similarity(generated_answer, ground_truth_answer)\n",
    "        \n",
    "        # Relevance score combines direct similarity and alignment with ground truth pattern\n",
    "        relevance_score = 0.7 * question_answer_similarity + 0.3 * answer_gt_similarity\n",
    "    else:\n",
    "        relevance_score = question_answer_similarity\n",
    "    \n",
    "    return {\n",
    "        \"relevance_score\": min(1.0, relevance_score),\n",
    "        \"question_similarity\": question_answer_similarity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "884d4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import numpy as np\n",
    "import re\n",
    "import logging\n",
    "from bert_score import score as bert_score\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import torch\n",
    "\n",
    "# Suppress all transformers warnings\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# More comprehensive warning suppression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"bert_score\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def evaluate_rag_system(test_questions, test_answers, llm_model, tokenizer, k=3):\n",
    "    \"\"\"\n",
    "    test_questions (list[str]): Input questions\n",
    "    test_answers (list[str]): Ground truth answers\n",
    "    llm_model: Preloaded language model (CPU)\n",
    "    tokenizer: Preloaded tokenizer\n",
    "    k (int): Top-k retrieved documents\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = []\n",
    "    latencies = []\n",
    "    cpu_times = []\n",
    "    memory_usages = []\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    \n",
    "    for i, (question, true_answer) in enumerate(zip(test_questions, test_answers)):\n",
    "        if i % 10 == 0:\n",
    "            print(f'Processing question {i}/{len(test_questions)}')\n",
    "            \n",
    "        # Start time\n",
    "        start_time = time.time()\n",
    "        cpu_start = process.cpu_times()\n",
    "        mem_start = process.memory_info().rss\n",
    "        \n",
    "        # Retrieve Context and generate answer\n",
    "        contexts_dicts = retriever(question, k=k)\n",
    "        \n",
    "        # Generate Answers\n",
    "        answer = generator(question, contexts_dicts,llm_model,tokenizer)\n",
    "        \n",
    "        # End timing and calculate metrics\n",
    "        end_time = time.time()\n",
    "        cpu_end = process.cpu_times()\n",
    "        mem_end = process.memory_info().rss\n",
    "        \n",
    "        latency = end_time - start_time\n",
    "        cpu_time = (cpu_end.user - cpu_start.user) + (cpu_end.system - cpu_start.system)\n",
    "        memory_usage = (mem_end - mem_start) / 1024 / 1024  # Convert to MB\n",
    "        \n",
    "        predictions.append(answer)\n",
    "        latencies.append(latency)\n",
    "        cpu_times.append(cpu_time)\n",
    "        memory_usages.append(max(0, memory_usage))\n",
    "\n",
    "    all_results = []\n",
    "    relevance_metrics=[]\n",
    " \n",
    "    \n",
    "    for i, (pred, true) in enumerate(zip(predictions, test_answers)):\n",
    "        if not pred or not true:\n",
    "            result = {\n",
    "                \"correctness_score\": 0.0,\n",
    "                \"semantic_similarity\": 0.0,\n",
    "                \"is_plausible\": 0.0,\n",
    "                \"substring_match\": 0.0,\n",
    "                \"bert_f1_score\": 0.0\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            relevance_metrics.append({\"relevance_score\":0,\"question_similarity\":0})\n",
    "            continue\n",
    "            \n",
    "        pred_norm = normalize_text(pred)\n",
    "        \n",
    "        # Get all the possible answers from value, aliases, normalized values\n",
    "        all_correct_answers = set()\n",
    "        all_correct_answers.add(true['value'])\n",
    "        all_correct_answers.update(true['aliases'])\n",
    "        all_correct_answers.update(true['normalized_aliases'])\n",
    "        all_correct_answers.add(true['normalized_value'])\n",
    "        \n",
    "        # Remove all the empty strings and normalize the correct answers\n",
    "        correct_answers_norm = [normalize_text(ans) for ans in all_correct_answers if ans and str(ans).strip()]\n",
    "        # Remove duplicates and empty\n",
    "        correct_answers_norm = list(set([ans for ans in correct_answers_norm if ans]))\n",
    "        \n",
    "        if not correct_answers_norm:\n",
    "            result = {\n",
    "                \"correctness_score\": 0.0,\n",
    "                \"semantic_similarity\": 0.0,\n",
    "                \"is_plausible\": 0.0,\n",
    "                \"substring_match\": 0.0,\n",
    "                \"bert_f1_score\": 0.0\n",
    "            }\n",
    "            all_results.append(result)\n",
    "            relevance_metrics.append({\"relevance_score\":0,\"question_similarity\":0})\n",
    "            continue\n",
    "        \n",
    "        # Check for substring match\n",
    "        substring_match = any(correct_norm in pred_norm for correct_norm in correct_answers_norm)\n",
    "        \n",
    "        # For semantic metrics use main value as the reference\n",
    "        ref_norm = normalize_text(true['value'])\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", message=\".*RobertaModel.*\")\n",
    "            P, R, F1 = bert_score([pred_norm], [ref_norm], lang='en', verbose=False)\n",
    "            bert_f1_score = float(F1[0])\n",
    "        \n",
    "        semantic_similarity = calculate_semantic_similarity(pred_norm,correct_answers_norm)\n",
    "        if semantic_similarity == 0.0 :\n",
    "            semantic_similarity = calculate_simple_similarity(pred_norm,correct_answers_norm)\n",
    "        \n",
    "        correctness_score = calculate_correctness(substring_match,semantic_similarity,bert_f1_score)\n",
    "        is_plausible = (semantic_similarity > 0.5 or substring_match)\n",
    "        \n",
    "               \n",
    "        #Calculate Answer Relevance\n",
    "        relevance_metric = calculate_answer_relevance(test_questions[i],pred,true['value'])\n",
    "        relevance_metrics.append(relevance_metric)\n",
    "        \n",
    "        text_correctness_results = {\n",
    "            \"correctness_score\": correctness_score,\n",
    "            \"semantic_similarity\": semantic_similarity,\n",
    "            \"is_plausible\": is_plausible,\n",
    "            \"substring_match\": substring_match,\n",
    "            \"bert_f1_score\": bert_f1_score,\n",
    "        }\n",
    "        all_results.append(text_correctness_results)\n",
    "    \n",
    "    # Calculate Average results\n",
    "    aggregated = {\n",
    "        \"total_items\": len(all_results),\n",
    "        \"average_correctness\": sum(r[\"correctness_score\"] for r in all_results) / len(all_results),\n",
    "        \"average_semantic_similarity\": sum(r[\"semantic_similarity\"] for r in all_results) / len(all_results),\n",
    "        \"plausible_count\": sum(r[\"is_plausible\"] for r in all_results),\n",
    "        \"plausible_percentage\": sum(r[\"is_plausible\"] for r in all_results) / len(all_results) * 100,\n",
    "        \"substring_match_count\": sum(r[\"substring_match\"] for r in all_results),\n",
    "        \"bert_f1_score\": sum(r[\"bert_f1_score\"] for r in all_results) / len(all_results),\n",
    "    }\n",
    "    \n",
    "   \n",
    "    # Aggregate relevance metrics\n",
    "    relevance_aggregated = {\n",
    "        \"relevance_score_avg\": sum(r[\"relevance_score\"] for r in relevance_metrics) / len(relevance_metrics),\n",
    "        \"question_similarity_avg\": sum(r[\"question_similarity\"] for r in relevance_metrics) / len(relevance_metrics),\n",
    "    }\n",
    "    \n",
    "    # Compile Results\n",
    "    results = {\n",
    "        \"latency_avg\": np.mean(latencies),\n",
    "        \"latency_std\": np.std(latencies),\n",
    "        \"cpu_time_avg\": np.mean(cpu_times),\n",
    "        \"cpu_time_std\": np.std(cpu_times),\n",
    "        \"memory_usage_avg\": np.mean(memory_usages),\n",
    "        \"average_correctness\": aggregated[\"average_correctness\"],\n",
    "        \"average_semantic_similarity\": aggregated[\"average_semantic_similarity\"],\n",
    "        \"plausible_percentage\": aggregated[\"plausible_percentage\"],\n",
    "        \"substring_match_count\": aggregated[\"substring_match_count\"],\n",
    "        \"bert_f1_score\": aggregated[\"bert_f1_score\"],\n",
    "        \n",
    "        # Relevance metrics\n",
    "        \"answer_relevance_score\": relevance_aggregated[\"relevance_score_avg\"],\n",
    "        \"question_answer_similarity\": relevance_aggregated[\"question_similarity_avg\"],\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab6188e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "def setup_logging(results: Dict[str, Any], test_items_count: int, log_dir: str = \"rag_evaluation_logs\"):\n",
    "    \"\"\"Setup logging with timestamp and test item count\"\"\"\n",
    "    \n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    main_folder=\"Logs\"\n",
    "    \n",
    "    # Create filename with timestamp and test count\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"rag_eval_{log_dir}_{timestamp}_{test_items_count}items.json\"\n",
    "    filepath = os.path.join(main_folder, filename)\n",
    "    \n",
    "    # Add metadata to results\n",
    "    results_with_metadata = {\n",
    "        \"metadata\": {\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"test_items_count\": test_items_count,\n",
    "            \"log_file\": filename\n",
    "        },\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_with_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Results logged to: {filepath}\")\n",
    "    return filepath\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a87a5f9",
   "metadata": {},
   "source": [
    "### Evaluate RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14a4af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Evaluation on test set.....\n",
      "Processing question 0/100\n",
      "Processing question 10/100\n",
      "Processing question 20/100\n",
      "Processing question 30/100\n",
      "Processing question 40/100\n",
      "Processing question 50/100\n",
      "Processing question 60/100\n",
      "Processing question 70/100\n",
      "Processing question 80/100\n",
      "Processing question 90/100\n",
      "Results logged to: Logs\\rag_eval_L6BM25_20250924_111429_100items.json\n",
      "\n",
      "=== RAG SYSTEM EVALUATION RESULTS ===\n",
      "Average latency : 5.7808 ± 1.1990 seconds\n",
      "Average CPU time : 44.60 ± 9.08 seconds\n",
      "Average Memory Usage : 6.1646 Mb\n",
      "\n",
      "=== RAG SYSTEM EVALUATION RESULTS FOR QUALITY OF ANSWER ===\n",
      "Average Correctness: 0.6095596061927473\n",
      "Average Semantic Similarity:0.22310214914381504\n",
      "Average Answer Relevance Score: 0.5731754459841175\n",
      "Average Question Answer Similarity: 0.7008134064078331\n",
      "Average Substring Matchcount:27\n",
      "Bert F1 Score: 0.80\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "#Create a random test set\n",
    "test_size = 100\n",
    "if len(questions) >= test_size:\n",
    "    indices = random.sample(range(len(questions)), test_size)\n",
    "    \n",
    "    test_questions = [questions[i] for i in indices]\n",
    "    test_answers = [answers[i] for i in indices]\n",
    "\n",
    "print(\"Running Evaluation on test set.....\")\n",
    "results = evaluate_rag_system(test_questions,test_answers,llm_model,tokenizer,k=3)\n",
    "setup_logging(results,len(test_questions),log_dir=\"L6BM25\")\n",
    "\n",
    "#Print Results\n",
    "print(\"\\n=== RAG SYSTEM EVALUATION RESULTS ===\")\n",
    "print(f\"Average latency : {results['latency_avg']:.4f} ± {results['latency_std']:.4f} seconds\")\n",
    "print(f\"Average CPU time : {results['cpu_time_avg']:.2f} ± {results['cpu_time_std']:.2f} seconds\")\n",
    "print(f\"Average Memory Usage : {results['memory_usage_avg']:.4f} Mb\")\n",
    "\n",
    "print(\"\\n=== RAG SYSTEM EVALUATION RESULTS FOR QUALITY OF ANSWER ===\")\n",
    "\n",
    "print(f\"Average Correctness: {results['average_correctness']}\")\n",
    "print(f\"Average Semantic Similarity:{results['average_semantic_similarity']}\")\n",
    "\n",
    "print(f\"Average Answer Relevance Score: {results['answer_relevance_score']}\")\n",
    "print(f\"Average Question Answer Similarity: {results['question_answer_similarity']}\")\n",
    "\n",
    "print(f\"Average Substring Matchcount:{results['substring_match_count']}\")\n",
    "print(f\"Bert F1 Score: {results['bert_f1_score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11cfee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46348c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
